{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# #sql code for We want to forecast for each item, what are the sales going to be next week\n# # orders with columns: order_id, order_date\n# # order_items with columns: order_id, product_id, quantity\n# # products with columns: product_id, product_name\n# #sql code\n# WITH WeeklySales AS (\n#     SELECT\n#         p.product_id,\n#         p.product_name,\n#         DATE_TRUNC('week', o.order_date) AS week_start,\n#         SUM(oi.quantity) AS weekly_sales\n#     FROM\n#         orders o\n#     JOIN\n#         order_items oi ON o.order_id = oi.order_id\n#     JOIN\n#         products p ON oi.product_id = p.product_id\n#     WHERE\n#         o.order_date >= CURRENT_DATE - INTERVAL '6 days'\n#         AND o.order_date < CURRENT_DATE + INTERVAL '1 day'\n#     GROUP BY\n#         p.product_id, p.product_name, week_start\n# )\n# , NextWeek AS (\n#     SELECT\n#         product_id,\n#         product_name,\n#         MAX(week_start) AS current_week_start\n#     FROM\n#         WeeklySales\n#     GROUP BY\n#         product_id, product_name\n# )\n# SELECT\n#     n.product_id,\n#     n.product_name,\n#     n.current_week_start + INTERVAL '1 week' AS next_week_start,\n#     COALESCE(w.weekly_sales, 0) AS forecasted_sales\n# FROM\n#     NextWeek n\n# LEFT JOIN\n#     WeeklySales w ON n.product_id = w.product_id\n#     AND n.current_week_start + INTERVAL '1 week' = w.week_start;\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-30T08:32:37.433439Z","iopub.execute_input":"2023-10-30T08:32:37.433884Z","iopub.status.idle":"2023-10-30T08:32:37.882387Z","shell.execute_reply.started":"2023-10-30T08:32:37.433848Z","shell.execute_reply":"2023-10-30T08:32:37.880953Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# To load relevant tables, perform efficient ETL, \n# and store the output in Parquet format, partitioned by product using Python \n# and pandas, you can follow the steps below. \n# I'll assume you already have the relevant CSV files for orders, order_items, and products.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install pandas pyarrow\n","metadata":{"execution":{"iopub.status.busy":"2023-10-31T06:45:28.129316Z","iopub.execute_input":"2023-10-31T06:45:28.129858Z","iopub.status.idle":"2023-10-31T06:46:05.177100Z","shell.execute_reply.started":"2023-10-31T06:45:28.129817Z","shell.execute_reply":"2023-10-31T06:46:05.175447Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.0.3)\nRequirement already satisfied: pyarrow in /opt/conda/lib/python3.10/site-packages (9.0.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3)\nRequirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.23.5)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport os\n\n# Define input CSV file paths\norders_path = 'path_to_orders.csv'\norder_items_path = 'path_to_order_items.csv'\nproducts_path = 'path_to_products.csv'\n\n# Define output directory for Parquet files\noutput_dir = 'output_directory/'\n\n# Load relevant tables into DataFrames\norders_df = pd.read_csv(orders_path)\norder_items_df = pd.read_csv(order_items_path)\nproducts_df = pd.read_csv(products_path)\n\n# Perform ETL to create a dataset for forecasting\n# Join tables to create a comprehensive dataset\nmerged_df = pd.merge(order_items_df, orders_df, on='order_id', how='inner')\nmerged_df = pd.merge(merged_df, products_df, on='product_id', how='inner')\n\n# Convert order_date to datetime\nmerged_df['order_date'] = pd.to_datetime(merged_df['order_date'])\n\n# Create a week_start column to be used for partitioning\nmerged_df['week_start'] = merged_df['order_date'] - pd.to_timedelta(merged_df['order_date'].dt.dayofweek, unit='D')\n\n# Calculate weekly sales\nweekly_sales = merged_df.groupby(['product_id', 'week_start'])['quantity'].sum().reset_index()\n\n# Create the output directory if it doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n# Save the dataset as a Parquet file partitioned by product\nweekly_sales.to_parquet(os.path.join(output_dir, 'sales_forecast.parquet'), \n                       engine='pyarrow', partition_cols=['product_id'])\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A couple of simple pytest tests, and run them in github actions at every PR.\n# Write Simple Pytest Tests\nimport pandas as pd\nimport pytest\nfrom your_etl_script import load_data, transform_data\n\ndef test_load_data():\n    # Create a sample DataFrame for testing\n    sample_data = pd.DataFrame({'product_id': [1, 2, 3], 'quantity': [10, 20, 30]})\n    \n    # Define your expected result DataFrame\n    expected_result = sample_data\n\n    # Replace with your actual load_data call\n    actual_result = load_data()\n    \n    pd.testing.assert_frame_equal(actual_result, expected_result)\n\ndef test_transform_data():\n    # Create a sample DataFrame for testing\n    sample_data = pd.DataFrame({'product_id': [1, 2, 3], 'quantity': [10, 20, 30]})\n    \n    # Define your expected result DataFrame after transformation\n    expected_result = sample_data  # Replace this with your expected result\n\n    # Replace with your actual transform_data call\n    actual_result = transform_data(sample_data)\n    \n    pd.testing.assert_frame_equal(actual_result, expected_result)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-31T06:48:56.473241Z","iopub.execute_input":"2023-10-31T06:48:56.473752Z","iopub.status.idle":"2023-10-31T06:48:57.665303Z","shell.execute_reply.started":"2023-10-31T06:48:56.473703Z","shell.execute_reply":"2023-10-31T06:48:57.663514Z"},"trusted":true},"execution_count":2,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytest\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myour_etl_script\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_data, transform_data\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_load_data\u001b[39m():\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Create a sample DataFrame for testing\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     sample_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_id\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquantity\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m30\u001b[39m]})\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'your_etl_script'"],"ename":"ModuleNotFoundError","evalue":"No module named 'your_etl_script'","output_type":"error"}]},{"cell_type":"code","source":"# Set Up GitHub Actions Workflow\nname: Run Tests on Pull Request\n\non:\n  pull_request:\n    branches:\n      - main  # Change this to your main branch name\n\njobs:\n  test:\n    name: Run Pytest Tests\n    runs-on: ubuntu-latest\n\n    steps:\n    - name: Checkout code\n      uses: actions/checkout@v2\n\n    - name: Set up Python\n      uses: actions/setup-python@v2\n      with:\n        python-version: 3.8\n\n    - name: Install dependencies\n      run: pip install -r requirements.txt  # Modify with the path to your requirements file\n\n    - name: Run Pytest\n      run: pytest\n        \n# Push Code and Create a Pull Request:\n\n# After writing your tests and setting up the \n# GitHub Actions workflow, commit and push your code to your GitHub repository.\n# Then create a pull request.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Configuration files in yml\n# 1. Application Configuration:\n\n# In this example, \n# you can have a configuration file for your application \n# that specifies settings such as database connections, API keys, and other parameters. Create an app_config.yml file:\ndatabase:\n  host: localhost\n  port: 5432\n  username: myuser\n  password: mypassword\n\napi_keys:\n  google: YOUR_GOOGLE_API_KEY\n  aws: YOUR_AWS_API_KEY\n\napp_settings:\n  debug: false\n    \n# 2. Environment Configuration:\n\n# You can use YAML for defining environment-specific configuration. Create an env_config.yml file:\nproduction:\n  database:\n    host: production-db-host\n    username: prod-user\n    password: prod-password\n\ndevelopment:\n  database:\n    host: development-db-host\n    username: dev-user\n    password: dev-password\n\ntesting:\n  database:\n    host: test-db-host\n    username: test-user\n    password: test-password\n        \n# 3. Database Connection Configuration:\n\n# In a database connection configuration file, you can specify the connection details for different databases:\nmysql_db:\n  host: localhost\n  port: 3306\n  username: mysqluser\n  password: mysqlpassword\n  database: mydatabase\n\npostgres_db:\n  host: localhost\n  port: 5432\n  username: postgresuser\n  password: postgrespassword\n  database: postgresdb\n    \n# 4. Docker Compose Configuration:\n\n# For defining a multi-container application setup using Docker Compose, you can use a docker-compose.yml file:\n\nversion: '3'\nservices:\n  web:\n    image: nginx:latest\n    ports:\n      - \"80:80\"\n  app:\n    image: myapp:latest\n    ports:\n      - \"5000:5000\"\n    environment:\n      DATABASE_URL: postgres://username:password@postgres_db/mydb\n  postgres_db:\n    image: postgres:latest\n    environment:\n      POSTGRES_USER: username\n      POSTGRES_PASSWORD: password\n      POSTGRES_DB: mydb\n\n# 5. CI/CD Pipeline Configuration:\n\n# For configuring a CI/CD pipeline, you can use YAML files. Below is an example of a GitHub Actions workflow file (.github/workflows/build.yml):        \n\nname: Build and Deploy\n\non:\n  push:\n    branches:\n      - main\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n\n    steps:\n    - name: Checkout code\n      uses: actions/checkout@v2\n\n    - name: Setup Node.js\n      uses: actions/setup-node@v2\n      with:\n        node-version: 14\n\n    - name: Install dependencies\n      run: npm install\n\n    - name: Build and Deploy\n      run: npm run deploy\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# How would you turn it into an application in production?","metadata":{},"execution_count":null,"outputs":[]}]}